
<p align="center">
  <img src="app/assets/banner_chm.png" width="100%" />
</p>





### Proyecto final â€“ Modelos Transformer aplicados a ImÃ¡genes

> ImplementaciÃ³n inspirada en el artÃ­culo  
> **â€œHigh-resolution canopy height maps by learning from airborne lidar and spaceborne GEDIâ€**  
> Repositorio original: https://github.com/facebookresearch/HighResCanopyHeight

---

## ğŸ“Œ Contexto y objetivo

El monitoreo forestal moderno necesita ir mÃ¡s allÃ¡ del â€œbosque / no bosqueâ€ y aproximarse a un **censo estructural del bosque**:  
- Â¿CuÃ¡nta Ã¡rea tiene Ã¡rboles?  
- Â¿QuÃ© tan altos son esos Ã¡rboles?  
- Â¿CÃ³mo se distribuye la altura del dosel en el territorio?

El artÃ­culo base propone un modelo capaz de **convertir imÃ¡genes satelitales RGB de muy alta resoluciÃ³n en mapas continuos de altura de dosel (~1 m)**, combinando informaciÃ³n de:

- **LiDAR aÃ©reo (ALS)** â†’ detalle fino, pero cobertura limitada.
- **LiDAR satelital GEDI** â†’ cobertura casi global, pero resoluciÃ³n (~25 m) y muestreo discreto (huellas).

Este proyecto reproduce y adapta ese enfoque usando **Transformers de visiÃ³n pre-entrenados**, y construye una interfaz en **Streamlit** para explorar:

- MÃ©tricas cientÃ­ficas (MAE, RMSE, RÂ², sesgo, altura P95).
- MÃ©tricas de **â€œcenso estructuralâ€** (altura promedio del dosel, % de Ã¡rea con Ã¡rboles, distribuciÃ³n de alturas, etc.).

---


## ğŸ—ï¸ Resumen teÃ³rico de la arquitectura

AquÃ­ se resume la arquitectura completa en 3 niveles: **encoder SSL**, **decoder ALS** y **modelo GEDI + fusiÃ³n**.

---

### 1. Encoder SSL: ViT Huge con DINOv2

<p align="center">
  <img src="app/assets/vit.png" width="100%" />
</p>


1. **Entrada**
   - ImÃ¡genes satelitales globales de 256Ã—256 pÃ­xeles.
   - Se genera un **multi-crop**:
     - 2 vistas **globales**.
     - 8 vistas **locales** (algunas con mÃ¡scara).

2. **TokenizaciÃ³n**
   - Cada imagen se divide en parches 16Ã—16 â†’ se aplanan a vectores.
   - Se proyectan a un embedding de dimensiÃ³n 1280 y se les suma un embedding posicional.

3. **Teacherâ€“Student (DINOv2)**
   - Dos ViT con la misma arquitectura:

<p align="center">
  <img src="app/assets/vit2.png" width="60%" />
</p>

     - **Student**: recibe vistas globales + locales (con masking). Se actualiza por gradiente.
     - **Teacher**: recibe vistas globales, se actualiza por EMA (promedio mÃ³vil de los pesos del student).
   - Las salidas del student intentan **imitar las del teacher** â†’ pÃ©rdida de auto-supervisiÃ³n.
   - Al final de esta fase nos quedamos con el **encoder entrenado**, no con un mapa de salida.

---

### 2. Decoder DPT para CHM de alta resoluciÃ³n (ALS)

<p align="center">
  <img src="app/assets/dpt.png" width="100%" />
</p>

A partir de aquÃ­, el encoder queda **congelado** y sÃ³lo se entrena el decoder.

1. **Reassemble blocks**

   - Toman las features del ViT en distintas capas y las transforman en mapas 2D a distintas escalas.

   <p align="center">
  <img src="app/assets/rem.png" width="60%" />
</p>

   - Cada bloque:
     - **Read**: reordena los tokens a su posiciÃ³n espacial â†’ mapa 2D.
     - **Concat + Project (Conv 1Ã—1)**: apila canales y reduce/reorganiza la informaciÃ³n.
     - **Resampleâ‚›**: ajusta el tamaÃ±o del mapa para trabajar en escalas 1/32, 1/16, 1/8 y 1/4.

2. **Fusion blocks**



   - Combinan informaciÃ³n **global** (mapas mÃ¡s pequeÃ±os) con **detalle fino** (mapas de mayor resoluciÃ³n).

<p align="center">
  <img src="app/assets/fus.png" width="50%" />
</p>

   - Cada bloque:
     - Aplica una **Residual Conv Unit** para limpiar/refinar.
     - **Suma residual** entre el mapa global y el mapa mÃ¡s fino.
     - Hace un **upsample Ã—2** (Resampleâ‚€.5) para ir subiendo de resoluciÃ³n.
     - Otro **Project (Conv 1Ã—1)** adapta el nÃºmero de canales para el siguiente nivel.

3. **Head (salida por bins)**

<p align="center">
  <img src="app/assets/head.png" width="60%" />
</p>

   - Toma el Ãºltimo mapa de features (64Ã—64) y:
     - Aplica un **upsample** para volver a 256Ã—256.
     - Conv 1Ã—1 â†’ genera **256 bins de altura por pÃ­xel**.
     - Softmax â†’ histograma de probabilidad de altura por pÃ­xel.
     - Promedio ponderado â†’ altura esperada en metros.
   - Se obtiene un **CHM predicho 256Ã—256**, alineado con el tile de entrada.

4. **FunciÃ³n de pÃ©rdida: Sigloss**


   - Variante de la pÃ©rdida de profundidad de Eigen et al.:
     - Trabaja en espacio logarÃ­tmico.
     - Penaliza errores absolutos y errores globales de escala.
   - Se usa el **CHM ALS real** como verdad terreno.

---

### 3. Modelo GEDI global y fusiÃ³n ALS + GEDI

<p align="center">
  <img src="app/assets/gedi.png" width="100%" />
</p>

1. **Modelo GEDI (CNN + metadata)**

   - Entrada:
     - Parche RGB de 128Ã—128.
     - Metadatos: latitud, longitud, elevaciÃ³n solar, Ã¡ngulo off-nadir, pendiente del terreno.
   - Arquitectura:
     - **Extractor CNN** con varias capas Conv2D + ReLU + MaxPooling.
     - **Flatten â†’ capas densas**, donde se concatenan los metadatos.
   - Salida:
     - Un escalar: altura **RH95** (GEDI) en ese footprint.

<p align="center">
  <img src="app/assets/cnn.png" width="100%" />
</p>

   - PÃ©rdida:
   
     - **L1 Loss** entre altura predicha y altura medida por GEDI.

2. **CÃ¡lculo de factor de reescalamiento**

   - Se cruzan las predicciones del modelo ALS y del modelo GEDI en zonas con datos comunes.
   - Se calcula un **factor de escala espacialmente suave** que corrige el CHM ALS.

<p align="center">
  <img src="app/assets/combi.png" width="100%" />
</p>

3. **CHM final**
   - El CHM ALS de alta resoluciÃ³n se multiplica por el factor de reescalamiento.
   - Resultado: **canopy height map continuo**, detallado y coherente con GEDI a escala global.

---



### ğŸ“ Estructura del repositorio

La organizaciÃ³n del proyecto estÃ¡ pensada para separar claramente la **lÃ³gica del modelo**, la **app de Streamlit**, los **pesos preentrenados** y la **configuraciÃ³n de despliegue con Docker**.

```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_landing_CHM_app.py  (Archivo principal de entrada de la app (landing / menÃº de navegaciÃ³n).)
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 1_MetodologÃ­a.py (PÃ¡gina donde se explica la metodologÃ­a del modelo, fases, arquitectura y flujo de datos.)
â”‚       â””â”€â”€ DemostraciÃ³n.py  (PÃ¡gina central de la demo interactiva.)
â”‚
â”œâ”€â”€ model/   (Contiene la lÃ³gica de alto nivel para el modelo y la inferencia:)
â”‚   â”œâ”€â”€ ssl_model.py
â”‚   â”œâ”€â”€ inference_neon_tile.py
â”‚   â”œâ”€â”€ inference_uploaded_pair.py
â”‚   â”œâ”€â”€ neon_data.py
â”‚   â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ models/   (implementaciones de bajo nivel reutilizadas del repositorio original del paper)
â”‚   â”œâ”€â”€ backbone.py
â”‚   â”œâ”€â”€ dpt_head.py
â”‚   â”œâ”€â”€ regressor.py
â”‚   â””â”€â”€ pl_modules/
â”‚       â””â”€â”€ ... (mÃ³dulos auxiliares de PyTorch Lightning)
â”‚
â”œâ”€â”€ saved_checkpoints/ (Guarda los pesos preentrenados del modelo CHM)
â”‚   â””â”€â”€ compressed_SSLhuge_aerial.pth
â”‚
â”œâ”€â”€ data/  (Contiene los recursos necesarios para reconstruir el NeonDataset) 
â”‚   â””â”€â”€ neon/
â”‚       â”œâ”€â”€ neon_tiles.csv
â”‚       â””â”€â”€ ... (rutas / referencias a los tiles NEON)
â”‚
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


### âš–ï¸ğŸŒ³ Descarga de pesos preentrenados 

Para que la aplicaciÃ³n pueda realizar **inferencia real**, es indispensable descargar los **pesos preentrenados** del modelo original de Meta AI:  
**High-Resolution Canopy Height Maps**.



#### Â¿De dÃ³nde descargar los pesos?

1. Ve al repositorio original del proyecto (Meta / `HighResolutionCanopyHeight`).
2. Busca la secciÃ³n de **model checkpoints / weights**.
3. Descarga, como mÃ­nimo, los siguientes archivos:

- âœ… **Checkpoint del modelo CHM**, por ejemplo:  
  `compressed_SSLhuge_aerial.pth`

- âœ… **Pesos de la red de normalizaciÃ³n RNet**, usados cuando `normtype = 2`.  
  El nombre del archivo debe coincidir con lo que espera la funciÃ³n  
  `load_rnet_normalizer()` en `model/ssl_model.py`.


#### 3.2. DÃ³nde ubicar los archivos descargados

Copia los archivos descargados en la carpeta:

```bash
saved_checkpoints/
```

---

### ğŸš€ EjecuciÃ³n del Proyecto con **Docker**
InstalaciÃ³n â€¢ Despliegue â€¢ Uso

Este proyecto estÃ¡ preparado para ejecutarse fÃ¡cilmente usando **Docker**, sin necesidad de instalar manualmente todas las dependencias en tu mÃ¡quina local.



#### ğŸ“ Clonar el repositorio

```bash
git clone https://github.com/StevenVegaL/HighRes-Canopy-Height
cd HighResCanopyHeightApp
```

âš ï¸ Importante:
Antes de continuar, asegÃºrate de que la carpeta saved_checkpoints/ contiene los pesos indicados en la secciÃ³n anterior (modelo CHM y RNet).


#### ğŸ› ï¸ Construir la imagen Docker
Desde la raÃ­z del proyecto, ejecuta:

```bash

docker build -t chm-demo .

```



ğŸ” Â¿QuÃ© hace este comando?

Elemento	DescripciÃ³n
-t chm-demo	Asigna el nombre chm-demo a la imagen Docker
.	Usa el Dockerfile ubicado en el directorio actual







####  â–¶ï¸ Ejecutar el contenedor
Una vez construida la imagen, puedes levantar el contenedor con:

```bash
docker run -p 8501:8501 chm-demo
```


ğŸ’¡ Si el puerto 8501 ya estÃ¡ ocupado en tu mÃ¡quina, puedes usar otro puerto externo, por ejemplo:

```bash
docker run -p 8502:8501 chm-demo
```




#### ğŸŒ Acceder a la aplicaciÃ³n
Con el contenedor en ejecuciÃ³n, abre tu navegador en:

```bash
http://localhost:8501
```
DeberÃ­as ver la landing de la aplicaciÃ³n.




---

### ğŸ’» EjecuciÃ³n local (opcional, sin Docker)

Aunque la forma recomendada de ejecutar el proyecto es mediante **Docker**, tambiÃ©n puedes correr la aplicaciÃ³n **localmente** si ya tienes **Python** instalado en tu mÃ¡quina.



#### ğŸ§¬ Crear entorno virtual e instalar dependencias

Se recomienda usar un entorno virtual para aislar las dependencias del proyecto.

#### 1ï¸âƒ£ Crear y activar el entorno virtual

```bash
python -m venv .venv
```

En Windows:

```bash
.venv\Scripts\activate
```

En Linux / macOS:

```bash

source .venv/bin/activate
```

VerÃ¡s que el prompt de tu terminal cambia, indicando que el entorno .venv estÃ¡ activo.

2ï¸âƒ£ Actualizar pip e instalar dependencias
Con el entorno virtual activado, ejecuta:

```bash
pip install --upgrade pip
pip install -r requirements.txt

```



#### ğŸš€  Lanzar la aplicaciÃ³n con Streamlit
Una vez instaladas las dependencias, desde la raÃ­z del proyecto ejecuta:

```bash

streamlit run app/streamlit_landing_CHM_app.py

```

Si todo estÃ¡ correctamente configurado (incluyendo los pesos en saved_checkpoints/), Streamlit levantarÃ¡ la aplicaciÃ³n.

ğŸŒ Acceder a la app
Abre tu navegador y visita:

```bash
http://localhost:8501
```

AllÃ­ ceberÃ­as ver la landing de la aplicaciÃ³n.

<p align="center">
  <img src="app/assets/lading.png" width="100%" />
</p>

---



### ğŸ§  ExplicaciÃ³n: Â¿cÃ³mo se cargan los pesos y cÃ³mo se realiza la inferencia?

La lÃ³gica de carga de pesos y de inferencia estÃ¡ dividida en dos contextos:

- #### ğŸŒ² Modo NEON (dataset) â€“ usa **RNet + NeonDataset**  
- #### ğŸ–¼ï¸ Modo de imagen subida â€“ usa solo el **modelo CHM con normalizaciÃ³n global**

En la aplicaciÃ³n de Streamlit implementÃ© estos **dos modos de uso**:

---

#### ğŸŒ² Modo NEON (dataset)



En este modo trabajo con **ejemplos internos del dataset NEON**, que es el mismo conjunto de datos que usa el artÃ­culo original.  
AquÃ­ **no** permito que el usuario suba cualquier imagen, sino que utilizo los **tiles definidos en el CSV** del repositorio oficial.

<p align="center">
  <img src="app/assets/neon.png" width="100%" />
</p>



El flujo es:

1. A travÃ©s de un **navegador de tiles** (Ã­ndice NEON), selecciono un recorte del dataset.


2. Con ese Ã­ndice, cargo:
   - La **imagen aÃ©rea RGB**.
   - El **CHM real** asociado (derivado de LiDAR).

<p align="center">
  <img src="app/assets/neon1.png" width="100%" />
</p>


3. Para la carga de datos reutilizo la misma lÃ³gica del paper:
   - Uso la clase `NeonDataset`.
   - Aplico la red de normalizaciÃ³n de dominio **RNet** para que las imÃ¡genes queden en el mismo espacio del entrenamiento.
4. Sobre la imagen normalizada paso el **modelo Transformer preentrenado (DINOv2 + DPT)** y obtengo el **mapa de altura predicho (CHM)**.
5. Como tambiÃ©n tengo el CHM real, en la interfaz puedo mostrar:
   - La **imagen aÃ©rea RGB**.
   - El **CHM predicho**.
   - El **CHM de referencia** (LiDAR).

6. Con ambos mapas (predicho vs real) calculo mÃ©tricas como:
   - **MAE**
   - **RMSE**
   - **RÂ² (pixel y por bloques)**
   - **Bias (sesgo medio)**

   <p align="center">
  <img src="app/assets/neon2.png" width="100%" />
</p>

De esta forma, el modo NEON reproduce de forma muy fiel el **pipeline original de evaluaciÃ³n** que se describe en el paper.

---

#### ğŸ–¼ï¸ Modo de imagen subida

<p align="center">
  <img src="app/assets/imagen.png" width="100%" />
</p>

El segundo modo es mÃ¡s flexible: la aplicaciÃ³n permite que el usuario suba un par de archivos:

- Una **imagen RGB** (vista aÃ©rea).
- Opcionalmente, el **CHM real** correspondiente a esa misma zona.


La idea es que estos archivos tengan caracterÃ­sticas similares a las de NEON (vista aÃ©rea, buena resoluciÃ³n, recortes tipo 256Ã—256, etc.).

El flujo es:

1. El usuario sube la imagen RGB (y opcionalmente el CHM real).

<p align="center">
  <img src="app/assets/imagen1.png" width="100%" />
</p>

2. La app verifica que:
   - La imagen sea **RGB (3 canales)**.
   - Si se sube CHM real, sus **dimensiones coincidan exactamente** con la predicciÃ³n que produce el modelo.
3. La imagen RGB pasa por el mismo **preprocesamiento**:
   - ConversiÃ³n a `float32` y normalizaciÃ³n a `[0, 1]`.
   - AplicaciÃ³n de la **normalizaciÃ³n global por canal** (los mismos `mean`/`std` del script original).
4. Esa imagen normalizada se pasa al **modelo CHM preentrenado**, que genera el mapa de altura predicho.
5. Si el usuario tambiÃ©n subiÃ³ un CHM real con el mismo tamaÃ±o, la app:
   - Compara **predicciÃ³n vs CHM real**.
   - Calcula nuevamente las **mÃ©tricas de error** (MAE, RMSE, RÂ², Bias, etc.).
6. Todos los resultados se muestran de forma interactiva en Streamlit, con:
   - ImÃ¡genes en formato RGB/colormap.
   - MÃ©tricas en tablas y tarjetas tipo â€œdashboardâ€.

<p align="center">
  <img src="app/assets/imagen2.png" width="100%" />
</p>

---

Con estos dos modos logro un equilibrio entre:

- Un **modo muy fiel al paper**, usando directamente el dataset NEON, su pipeline y sus mÃ©tricas originales.
- Un **modo de experimentaciÃ³n**, donde se pueden evaluar pares de datos externos que respeten condiciones similares (imagen RGB + CHM real), pero todo presentado de forma mÃ¡s **visual e interactiva** en Streamlit.























---

## ğŸ“š Referencias

- Weinstein, B. G., et al. **High-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI.**  
- Repositorio oficial: https://github.com/facebookresearch/HighResCanopyHeight

