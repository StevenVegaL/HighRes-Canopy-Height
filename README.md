
<p align="center">
  <img src="app/assets/banner_chm.png" width="100%" />
</p>





### Proyecto final ‚Äì Modelos Transformer aplicados a Im√°genes

> Implementaci√≥n inspirada en el art√≠culo  
> **‚ÄúHigh-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI‚Äù**  
> Repositorio original: https://github.com/facebookresearch/HighResCanopyHeight

---

## üìå Contexto y objetivo del proyecto

El monitoreo forestal es indispensable para la gesti√≥n de recursos naturales y la comprensi√≥n del cambio clim√°tico.  
Para hacerlo bien, no basta con un mapa binario ‚Äúbosque / no bosque‚Äù: se necesita un **censo estructural del bosque**, es decir, saber:

- cu√°nta √°rea tiene √°rboles,
- qu√© tan altos son esos √°rboles,
- y c√≥mo se distribuye la altura del dosel en el territorio.

Hoy existen herramientas de teledetecci√≥n potentes, como el **LiDAR (l√°ser)** y las c√°maras √≥pticas de alta resoluci√≥n, que permiten reconstruir bastante bien la estructura forestal.  
Sin embargo, las soluciones de alcance global presentan limitaciones importantes. En particular:

- El **LiDAR satelital GEDI** (NASA) ofrece cobertura casi global,  
  pero con **baja resoluci√≥n (~25 m)** y datos **discretos** en forma de huellas,  
  no como un mapa continuo.
- El **LiDAR a√©reo (ALS)** s√≠ permite mapas muy detallados,  
  pero solo existe para campa√±as puntuales y regiones concretas.

Esto genera un **vac√≠o de datos continuos de muy alta resoluci√≥n (VHR)** sobre la altura del dosel: justo el tipo de informaci√≥n que se necesitar√≠a como base para un censo forestal estructural detallado.

A partir de este contexto, la pregunta problema que abordamos es:

> **¬øC√≥mo generar, a partir de im√°genes satelitales RGB de alta resoluci√≥n, mapas continuos y precisos de altura del dosel que sirvan de base a un censo forestal estructural de detalle?**

Para cerrar este vac√≠o, tomamos como referencia el art√≠culo:

> *‚ÄúHigh-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI‚Äù* (Meta / FAIR)

Este trabajo propone convertir im√°genes √≥pticas de muy alta resoluci√≥n en **mapas continuos de altura por p√≠xel (~1 m)**, combinando:

- **LiDAR a√©reo (ALS)** ‚Üí detalle fino y estructura local del dosel.  
- **LiDAR satelital GEDI** ‚Üí contexto global y calibraci√≥n de alturas.

En este repositorio:

- Reproducimos y adaptamos el modelo basado en *Vision Transformers* y *Dense Prediction Transformer (DPT)*.  
- Utilizamos los modelos preentrenados publicados por los autores (SSL + ALS + GEDI).  
- Construimos una interfaz en **Streamlit** que permite:
  - explorar ejemplos reales del dataset NEON,  
  - cargar pares propios (RGB + CHM) para evaluaci√≥n,  
  - y visualizar de forma interactiva mapas de altura del dosel que pueden usarse como base para un censo estructural (altura promedio, distribuci√≥n de alturas, etc.).


---


## üèóÔ∏è Resumen te√≥rico de la arquitectura

Aqu√≠ se resume la arquitectura completa en 3 niveles: **encoder SSL**, **decoder ALS** y **modelo GEDI + fusi√≥n**.

---

### 1. Encoder SSL: ViT Huge con DINOv2

<p align="center">
  <img src="app/assets/vit.png" width="100%" />
</p>


1. **Entrada**
   - Im√°genes satelitales globales de 256√ó256 p√≠xeles.
   - Se genera un **multi-crop**:
     - 2 vistas **globales**.
     - 8 vistas **locales** (algunas con m√°scara).

2. **Tokenizaci√≥n**
   - Cada imagen se divide en parches 16√ó16 ‚Üí se aplanan a vectores.
   - Se proyectan a un embedding de dimensi√≥n 1280 y se les suma un embedding posicional.

3. **Teacher‚ÄìStudent (DINOv2)**
   - Dos ViT con la misma arquitectura:

<p align="center">
  <img src="app/assets/vit2.png" width="60%" />
</p>


  - **Student**: recibe vistas globales + locales (con masking). Se actualiza por gradiente.
  - **Teacher**: recibe vistas globales, se actualiza por EMA (promedio m√≥vil de los pesos del student).
  - Las salidas del student intentan **imitar las del teacher** ‚Üí p√©rdida de auto-supervisi√≥n.
  - Al final de esta fase nos quedamos con el **encoder entrenado**, no con un mapa de salida.

---

### 2. Decoder DPT para CHM de alta resoluci√≥n (ALS)

<p align="center">
  <img src="app/assets/dpt.png" width="100%" />
</p>

A partir de aqu√≠, el encoder queda **congelado** y s√≥lo se entrena el decoder.

1. **Reassemble blocks**

   - Toman las features del ViT en distintas capas y las transforman en mapas 2D a distintas escalas.

   <p align="center">
  <img src="app/assets/rem.png" width="60%" />
</p>

   - Cada bloque:
     - **Read**: reordena los tokens a su posici√≥n espacial ‚Üí mapa 2D.
     - **Concat + Project (Conv 1√ó1)**: apila canales y reduce/reorganiza la informaci√≥n.
     - **Resample‚Çõ**: ajusta el tama√±o del mapa para trabajar en escalas 1/32, 1/16, 1/8 y 1/4.

2. **Fusion blocks**



   - Combinan informaci√≥n **global** (mapas m√°s peque√±os) con **detalle fino** (mapas de mayor resoluci√≥n).

<p align="center">
  <img src="app/assets/fus.png" width="50%" />
</p>

   - Cada bloque:
     - Aplica una **Residual Conv Unit** para limpiar/refinar.
     - **Suma residual** entre el mapa global y el mapa m√°s fino.
     - Hace un **upsample √ó2** (Resample‚ÇÄ.5) para ir subiendo de resoluci√≥n.
     - Otro **Project (Conv 1√ó1)** adapta el n√∫mero de canales para el siguiente nivel.

3. **Head (salida por bins)**

<p align="center">
  <img src="app/assets/head.png" width="60%" />
</p>

   - Toma el √∫ltimo mapa de features (64√ó64) y:
     - Aplica un **upsample** para volver a 256√ó256.
     - Conv 1√ó1 ‚Üí genera **256 bins de altura por p√≠xel**.
     - Softmax ‚Üí histograma de probabilidad de altura por p√≠xel.
     - Promedio ponderado ‚Üí altura esperada en metros.
   - Se obtiene un **CHM predicho 256√ó256**, alineado con el tile de entrada.

4. **Funci√≥n de p√©rdida: Sigloss**


   - Variante de la p√©rdida de profundidad de Eigen et al.:
     - Trabaja en espacio logar√≠tmico.
     - Penaliza errores absolutos y errores globales de escala.
   - Se usa el **CHM ALS real** como verdad terreno.

---

### 3. Modelo GEDI global y fusi√≥n ALS + GEDI

<p align="center">
  <img src="app/assets/gedi.png" width="100%" />
</p>

1. **Modelo GEDI (CNN + metadata)**

   - Entrada:
     - Parche RGB de 128√ó128.
     - Metadatos: latitud, longitud, elevaci√≥n solar, √°ngulo off-nadir, pendiente del terreno.
   - Arquitectura:
     - **Extractor CNN** con varias capas Conv2D + ReLU + MaxPooling.
     - **Flatten ‚Üí capas densas**, donde se concatenan los metadatos.
   - Salida:
     - Un escalar: altura **RH95** (GEDI) en ese footprint.

<p align="center">
  <img src="app/assets/cnn.png" width="100%" />
</p>

   - P√©rdida:
   
     - **L1 Loss** entre altura predicha y altura medida por GEDI.

2. **C√°lculo de factor de reescalamiento**

   - Se cruzan las predicciones del modelo ALS y del modelo GEDI en zonas con datos comunes.
   - Se calcula un **factor de escala espacialmente suave** que corrige el CHM ALS.

<p align="center">
  <img src="app/assets/combi.png" width="100%" />
</p>

3. **CHM final**
   - El CHM ALS de alta resoluci√≥n se multiplica por el factor de reescalamiento.
   - Resultado: **canopy height map continuo**, detallado y coherente con GEDI a escala global.

---



## üìÅ Estructura del repositorio

La organizaci√≥n del proyecto est√° pensada para separar claramente la **l√≥gica del modelo**, la **app de Streamlit**, los **pesos preentrenados** y la **configuraci√≥n de despliegue con Docker**.

```bash
.
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_landing_CHM_app.py  (Archivo principal de entrada de la app (landing / men√∫ de navegaci√≥n).)
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ       ‚îú‚îÄ‚îÄ 1_Metodolog√≠a.py (P√°gina donde se explica la metodolog√≠a del modelo, fases, arquitectura y flujo de datos.)
‚îÇ       ‚îî‚îÄ‚îÄ Demostraci√≥n.py  (P√°gina central de la demo interactiva.)
‚îÇ
‚îú‚îÄ‚îÄ model/   (Contiene la l√≥gica de alto nivel para el modelo y la inferencia:)
‚îÇ   ‚îú‚îÄ‚îÄ ssl_model.py
‚îÇ   ‚îú‚îÄ‚îÄ inference_neon_tile.py
‚îÇ   ‚îú‚îÄ‚îÄ inference_uploaded_pair.py
‚îÇ   ‚îú‚îÄ‚îÄ neon_data.py
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py
‚îÇ
‚îú‚îÄ‚îÄ models/   (implementaciones de bajo nivel reutilizadas del repositorio original del paper)
‚îÇ   ‚îú‚îÄ‚îÄ backbone.py
‚îÇ   ‚îú‚îÄ‚îÄ dpt_head.py
‚îÇ   ‚îú‚îÄ‚îÄ regressor.py
‚îÇ   ‚îî‚îÄ‚îÄ pl_modules/
‚îÇ       ‚îî‚îÄ‚îÄ ... (m√≥dulos auxiliares de PyTorch Lightning)
‚îÇ
‚îú‚îÄ‚îÄ saved_checkpoints/ (Guarda los pesos preentrenados del modelo CHM)
‚îÇ   ‚îî‚îÄ‚îÄ compressed_SSLhuge_aerial.pth
‚îÇ
‚îú‚îÄ‚îÄ data/  (Contiene los recursos necesarios para reconstruir el NeonDataset) 
‚îÇ   ‚îî‚îÄ‚îÄ neon/
‚îÇ       ‚îú‚îÄ‚îÄ neon_tiles.csv
‚îÇ       ‚îî‚îÄ‚îÄ ... (rutas / referencias a los tiles NEON)
‚îÇ
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öñÔ∏èüå≥ Descarga de pesos preentrenados 

Para que la aplicaci√≥n pueda realizar **inferencia real**, es indispensable descargar los **pesos preentrenados** del modelo original de Meta AI:  
**High-Resolution Canopy Height Maps**.


#### ¬øDe d√≥nde descargar los pesos?

Para mantenernos alineados con el repositorio original de Meta, los datos y modelos preentrenados se descargan desde un bucket p√∫blico de Amazon S3:

`S3: s3://dataforgood-fb-data/forests/v1/models/`

Desde la ra√≠z del repositorio (la carpeta donde est√° este `README.md`), ejecuta en la terminal:

```bash
aws s3 --no-sign-request cp --recursive s3://dataforgood-fb-data/forests/v1/models/ .
unzip data.zip
rm data.zip
```

> ; Esto har√° tres cosas:

1. Descargar todo el contenido del bucket (datos + checkpoints) a tu m√°quina.
2. Descomprimir data.zip (que contiene, entre otros, las im√°genes a√©reas de NEON).
3. Eliminar el .zip para no ocupar espacio extra.

üí° **Requisito:** debes tener instalado el AWS CLI (`aws`) y ejecutarlo desde la ra√≠z del proyecto.

---

#### ¬øQu√© archivos son importantes para este proyecto?

Despu√©s de correr los comandos anteriores, en la carpeta `saved_checkpoints/` tendr√°s varios modelos.  
Para esta app de Streamlit, los m√°s relevantes son:

- `compressed_SSLhuge_aerial.pth`

  Encoder entrenado en im√°genes satelitales y decoder entrenado en im√°genes a√©reas.

  Es el checkpoint que usamos para inferencia en la aplicaci√≥n (modo NEON / modo imagen subida).

- **Pesos de la red de normalizaci√≥n RNet**

  Se utilizan cuando normtype = 2.

  El nombre del archivo debe coincidir con el que espera la funci√≥n  
  `load_rnet_normalizer()` en `model/ssl_model.py`.

  Importante: no renombres los archivos, para que el c√≥digo lo encuentre sin problemas.

---

#### ¬øD√≥nde ubicar los archivos descargados?

Todos los checkpoints deben quedar en la carpeta:

`saved_checkpoints/`

Si usaste los comandos de nombrados, esta carpeta se crea autom√°ticamente y los archivos ya quedar√°n en la ubicaci√≥n correcta, por lo que no es necesario moverlos a mano.


---

## üöÄ Ejecuci√≥n del Proyecto con **Docker**
Instalaci√≥n ‚Ä¢ Despliegue ‚Ä¢ Uso

Este proyecto est√° preparado para ejecutarse f√°cilmente usando **Docker**, sin necesidad de instalar manualmente todas las dependencias en tu m√°quina local.



#### üìÅ Clonar el repositorio

```bash
git clone https://github.com/StevenVegaL/HighRes-Canopy-Height
cd HighResCanopyHeightApp
```

‚ö†Ô∏è Importante:
Antes de continuar, aseg√∫rate de que la carpeta saved_checkpoints/ contiene los pesos indicados en la secci√≥n anterior (modelo CHM y RNet).


#### üõ†Ô∏è Construir la imagen Docker
Desde la ra√≠z del proyecto, ejecuta:

```bash

docker build -t chm-demo .

```



üîé ¬øQu√© hace este comando?

Elemento	Descripci√≥n
-t chm-demo	Asigna el nombre chm-demo a la imagen Docker
.	Usa el Dockerfile ubicado en el directorio actual







####  ‚ñ∂Ô∏è Ejecutar el contenedor
Una vez construida la imagen, puedes levantar el contenedor con:

```bash
docker run -p 8501:8501 chm-demo
```


üí° Si el puerto 8501 ya est√° ocupado en tu m√°quina, puedes usar otro puerto externo, por ejemplo:

```bash
docker run -p 8502:8501 chm-demo
```




#### üåê Acceder a la aplicaci√≥n
Con el contenedor en ejecuci√≥n, abre tu navegador en:

```bash
http://localhost:8501
```
Deber√≠as ver la landing de la aplicaci√≥n.




---

## üíª Ejecuci√≥n local (opcional, sin Docker)

Aunque la forma recomendada de ejecutar el proyecto es mediante **Docker**, tambi√©n puedes correr la aplicaci√≥n **localmente** si ya tienes **Python** instalado en tu m√°quina.



#### üß¨ Crear entorno virtual e instalar dependencias

Se recomienda usar un entorno virtual para aislar las dependencias del proyecto.

#### 1Ô∏è‚É£ Crear y activar el entorno virtual

```bash
python -m venv .venv
```

En Windows:

```bash
.venv\Scripts\activate
```

En Linux / macOS:

```bash

source .venv/bin/activate
```

Ver√°s que el prompt de tu terminal cambia, indicando que el entorno .venv est√° activo.

2Ô∏è‚É£ Actualizar pip e instalar dependencias
Con el entorno virtual activado, ejecuta:

```bash
pip install --upgrade pip
pip install -r requirements.txt

```



#### üöÄ  Lanzar la aplicaci√≥n con Streamlit
Una vez instaladas las dependencias, desde la ra√≠z del proyecto ejecuta:

```bash

streamlit run app/streamlit_landing_CHM_app.py

```

Si todo est√° correctamente configurado (incluyendo los pesos en saved_checkpoints/), Streamlit levantar√° la aplicaci√≥n.

üåê Acceder a la app
Abre tu navegador y visita:

```bash
http://localhost:8501
```

All√≠ ceber√≠as ver la landing de la aplicaci√≥n.

<p align="center">
  <img src="app/assets/landing.png" width="100%" />
</p>

---



## üß† ¬øc√≥mo se realiza la inferencia?

La l√≥gica de carga de pesos y de inferencia est√° dividida en dos contextos:

- #### üå≤ Modo NEON (dataset) ‚Äì usa **RNet + NeonDataset**  
- #### üñºÔ∏è Modo de imagen subida ‚Äì usa solo el **modelo CHM con normalizaci√≥n global**

En la aplicaci√≥n de Streamlit implement√© estos **dos modos de uso**:

---

### üå≤ Modo NEON (dataset)



En este modo trabajo con **ejemplos internos del dataset NEON**, que es el mismo conjunto de datos que usa el art√≠culo original.  


<p align="center">
  <img src="app/assets/neon.png" width="100%" />
</p>


Aqu√≠ **no** permito que el usuario suba cualquier imagen, sino que utilizo los **tiles definidos en el CSV** del repositorio oficial.

El flujo es:

1. A trav√©s de un **navegador de tiles** (√≠ndice NEON), selecciono un recorte del dataset.


2. Con ese √≠ndice, cargo:
   - La **imagen a√©rea RGB**.
   - El **CHM real** asociado (derivado de LiDAR).

<p align="center">
  <img src="app/assets/neon1.png" width="100%" />
</p>


3. Para la carga de datos reutilizo la misma l√≥gica del paper:
   - Uso la clase `NeonDataset`.
   - Aplico la red de normalizaci√≥n de dominio **RNet** para que las im√°genes queden en el mismo espacio del entrenamiento.
4. Sobre la imagen normalizada paso el **modelo Transformer preentrenado (DINOv2 + DPT)** y obtengo el **mapa de altura predicho (CHM)**.
5. Como tambi√©n tengo el CHM real, en la interfaz puedo mostrar:
   - La **imagen a√©rea RGB**.
   - El **CHM predicho**.
   - El **CHM de referencia** (LiDAR).

6. Con ambos mapas (predicho vs real) calculo m√©tricas como:
   - **MAE**
   - **RMSE**
   - **R¬≤ (pixel y por bloques)**
   - **Bias (sesgo medio)**

   <p align="center">
  <img src="app/assets/neon2.png" width="100%" />
</p>

De esta forma, el modo NEON reproduce de forma muy fiel el **pipeline original de evaluaci√≥n** que se describe en el paper.

---

### üñºÔ∏è Modo de imagen subida

<p align="center">
  <img src="app/assets/imagen.png" width="100%" />
</p>

El segundo modo es m√°s flexible: la aplicaci√≥n permite que el usuario suba un par de archivos:

- Una **imagen RGB** (vista a√©rea).
- Opcionalmente, el **CHM real** correspondiente a esa misma zona.


La idea es que estos archivos tengan caracter√≠sticas similares a las de NEON (vista a√©rea, buena resoluci√≥n, recortes tipo 256√ó256, etc.).

El flujo es:

1. El usuario sube la imagen RGB (y opcionalmente el CHM real).

<p align="center">
  <img src="app/assets/imagen1.png" width="100%" />
</p>

2. La app verifica que:
   - La imagen sea **RGB (3 canales)**.
   - Si se sube CHM real, sus **dimensiones coincidan exactamente** con la predicci√≥n que produce el modelo.
3. La imagen RGB pasa por el mismo **preprocesamiento**:
   - Conversi√≥n a `float32` y normalizaci√≥n a `[0, 1]`.
   - Aplicaci√≥n de la **normalizaci√≥n global por canal** (los mismos `mean`/`std` del script original).
4. Esa imagen normalizada se pasa al **modelo CHM preentrenado**, que genera el mapa de altura predicho.
5. Si el usuario tambi√©n subi√≥ un CHM real con el mismo tama√±o, la app:
   - Compara **predicci√≥n vs CHM real**.
   - Calcula nuevamente las **m√©tricas de error** (MAE, RMSE, R¬≤, Bias, etc.).
6. Todos los resultados se muestran de forma interactiva en Streamlit, con:
   - Im√°genes en formato RGB/colormap.
   - M√©tricas en tablas y tarjetas tipo ‚Äúdashboard‚Äù.

<p align="center">
  <img src="app/assets/imagen2.png" width="100%" />
</p>

---

Con estos dos modos logramos un equilibrio entre:

- Un **modo muy fiel al paper**, usando directamente el dataset NEON, su pipeline y sus m√©tricas originales.
- Un **modo de experimentaci√≥n**, donde se pueden evaluar pares de datos externos que respeten condiciones similares (imagen RGB + CHM real), pero todo presentado de forma m√°s **visual e interactiva** en Streamlit.









---

## üìö Referencias

- Weinstein, B. G., et al. **High-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI.**  
- Repositorio oficial: https://github.com/facebookresearch/HighResCanopyHeight

