# üå≥ High-Resolution Canopy Height  


<p align="center">
  <img src="assets/banner_chm.png" width="90%" />
</p>




### Proyecto final ‚Äì Modelos Transformer aplicados a Im√°genes

> Implementaci√≥n inspirada en el art√≠culo  
> **‚ÄúHigh-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI‚Äù**  
> Repositorio original: https://github.com/facebookresearch/HighResCanopyHeight

---

## üìå Contexto y objetivo

El monitoreo forestal moderno necesita ir m√°s all√° del ‚Äúbosque / no bosque‚Äù y aproximarse a un **censo estructural del bosque**:  
- ¬øCu√°nta √°rea tiene √°rboles?  
- ¬øQu√© tan altos son esos √°rboles?  
- ¬øC√≥mo se distribuye la altura del dosel en el territorio?

El art√≠culo base propone un modelo capaz de **convertir im√°genes satelitales RGB de muy alta resoluci√≥n en mapas continuos de altura de dosel (~1 m)**, combinando informaci√≥n de:

- **LiDAR a√©reo (ALS)** ‚Üí detalle fino, pero cobertura limitada.
- **LiDAR satelital GEDI** ‚Üí cobertura casi global, pero resoluci√≥n (~25 m) y muestreo discreto (huellas).

Este proyecto reproduce y adapta ese enfoque usando **Transformers de visi√≥n pre-entrenados**, y construye una interfaz en **Streamlit** para explorar:

- M√©tricas cient√≠ficas (MAE, RMSE, R¬≤, sesgo, altura P95).
- M√©tricas de **‚Äúcenso estructural‚Äù** (altura promedio del dosel, % de √°rea con √°rboles, distribuci√≥n de alturas, etc.).

---

## üß† Descripci√≥n del modelo e innovaciones principales

El coraz√≥n del art√≠culo (y de este repo) es un pipeline en varias fases:

1. **Pre-entrenamiento auto-supervisado (SSL) en im√°genes satelitales**
   - Se usa un **ViT Huge** (Vision Transformer) pre-entrenado con **DINOv2** sobre **18 millones de im√°genes satelitales Maxar**.
   - El modelo aprende a ‚Äúentender‚Äù texturas de bosque, bordes de copas, sombras, caminos, etc. **sin etiquetas de altura**.
   - Resultado: un **encoder especializado en vegetaci√≥n y paisaje**, que luego se reutiliza como backbone.

2. **Decoder DPT para altura de dosel de alta resoluci√≥n (ALS)**
   - Encima del encoder congelado se entrena un **decoder DPT (Dense Prediction Transformer)**.
   - Entrada: im√°genes RGB de sitios NEON (1 m GSD).  
   - Salida: mapas de altura de dosel (CHM) a la misma resoluci√≥n.
   - Se utiliza:
     - **Arquitectura multi-escala (Reassemble + Fusion blocks)** para combinar contexto global y detalle fino.
     - **P√©rdida Sigloss (tipo profundidad)** y **salida por bins (256 contenedores de altura)** para mejorar la estabilidad y evitar sesgos hacia alturas peque√±as.

3. **Modelo GEDI global (CNN + metadata)**
   - Se entrena un modelo separado (CNN) que:
     - Recibe parches RGB de 128√ó128.
     - Usa metadatos del haz GEDI: latitud, longitud, elevaci√≥n solar, √°ngulo off-nadir y pendiente del terreno.
   - Predice la altura **RH95** (percentil 95 de altura) en el footprint de GEDI.
   - Esto permite tener un modelo consistente con las mediciones **globales** de GEDI, aunque sean de baja resoluci√≥n.

4. **Fusi√≥n ALS + GEDI: mapa ajustado a escala global**
   - El mapa de CHM de alta resoluci√≥n que se obtuvo con ALS se corrige usando el modelo GEDI:
     - El modelo GEDI act√∫a como una referencia global de ‚Äúescala‚Äù de altura.
     - Se calcula un **factor de reescalamiento espacialmente variable** que ajusta el CHM ALS hacia la escala de GEDI.
   - Resultado: un **mapa continuo de altura de dosel**, con detalle de ~1 m, pero coherente con las alturas observadas por GEDI a escala global.

üîç **Innovaciones clave:**

- Uso de **Transformers de visi√≥n pre-entrenados auto-supervisados** espec√≠ficamente sobre **im√°genes satelitales**, no solo datos gen√©ricos tipo ImageNet.
- Arquitectura **DPT multi-escala** adaptada a mapas de altura de dosel:
  - Combina vista global del bosque y detalle de copas individuales.
- **Salida por bins + Sigloss**:
  - El modelo no predice directamente un escalar, sino una distribuci√≥n discreta de alturas que luego se convierte en altura esperada.
  - Mejora estabilidad y reduce sesgos.
- **Fusi√≥n ALS + GEDI** para lograr:
  - Detalle local (ALS) + coherencia global (GEDI) en un solo CHM continuo.

---

## üèóÔ∏è Resumen te√≥rico de la arquitectura

Aqu√≠ se resume la arquitectura completa en 3 niveles: **encoder SSL**, **decoder ALS** y **modelo GEDI + fusi√≥n**.

---

### 1. Encoder SSL: ViT Huge con DINOv2

1. **Entrada**
   - Im√°genes satelitales globales de 256√ó256 p√≠xeles.
   - Se genera un **multi-crop**:
     - 2 vistas **globales**.
     - 8 vistas **locales** (algunas con m√°scara).

2. **Tokenizaci√≥n**
   - Cada imagen se divide en parches 16√ó16 ‚Üí se aplanan a vectores.
   - Se proyectan a un embedding de dimensi√≥n 1280 y se les suma un embedding posicional.

3. **Teacher‚ÄìStudent (DINOv2)**
   - Dos ViT con la misma arquitectura:
     - **Student**: recibe vistas globales + locales (con masking). Se actualiza por gradiente.
     - **Teacher**: recibe vistas globales, se actualiza por EMA (promedio m√≥vil de los pesos del student).
   - Las salidas del student intentan **imitar las del teacher** ‚Üí p√©rdida de auto-supervisi√≥n.
   - Al final de esta fase nos quedamos con el **encoder entrenado**, no con un mapa de salida.

---

### 2. Decoder DPT para CHM de alta resoluci√≥n (ALS)

A partir de aqu√≠, el encoder queda **congelado** y s√≥lo se entrena el decoder.

1. **Reassemble blocks**
   - Toman las features del ViT en distintas capas y las transforman en mapas 2D a distintas escalas.
   - Cada bloque:
     - **Read**: reordena los tokens a su posici√≥n espacial ‚Üí mapa 2D.
     - **Concat + Project (Conv 1√ó1)**: apila canales y reduce/reorganiza la informaci√≥n.
     - **Resample‚Çõ**: ajusta el tama√±o del mapa para trabajar en escalas 1/32, 1/16, 1/8 y 1/4.

2. **Fusion blocks**
   - Combinan informaci√≥n **global** (mapas m√°s peque√±os) con **detalle fino** (mapas de mayor resoluci√≥n).
   - Cada bloque:
     - Aplica una **Residual Conv Unit** para limpiar/refinar.
     - **Suma residual** entre el mapa global y el mapa m√°s fino.
     - Hace un **upsample √ó2** (Resample‚ÇÄ.5) para ir subiendo de resoluci√≥n.
     - Otro **Project (Conv 1√ó1)** adapta el n√∫mero de canales para el siguiente nivel.

3. **Head (salida por bins)**
   - Toma el √∫ltimo mapa de features (64√ó64) y:
     - Aplica un **upsample** para volver a 256√ó256.
     - Conv 1√ó1 ‚Üí genera **256 bins de altura por p√≠xel**.
     - Softmax ‚Üí histograma de probabilidad de altura por p√≠xel.
     - Promedio ponderado ‚Üí altura esperada en metros.
   - Se obtiene un **CHM predicho 256√ó256**, alineado con el tile de entrada.

4. **Funci√≥n de p√©rdida: Sigloss**
   - Variante de la p√©rdida de profundidad de Eigen et al.:
     - Trabaja en espacio logar√≠tmico.
     - Penaliza errores absolutos y errores globales de escala.
   - Se usa el **CHM ALS real** como verdad terreno.

---

### 3. Modelo GEDI global y fusi√≥n ALS + GEDI

1. **Modelo GEDI (CNN + metadata)**
   - Entrada:
     - Parche RGB de 128√ó128.
     - Metadatos: latitud, longitud, elevaci√≥n solar, √°ngulo off-nadir, pendiente del terreno.
   - Arquitectura:
     - **Extractor CNN** con varias capas Conv2D + ReLU + MaxPooling.
     - **Flatten ‚Üí capas densas**, donde se concatenan los metadatos.
   - Salida:
     - Un escalar: altura **RH95** (GEDI) en ese footprint.
   - P√©rdida:
     - **L1 Loss** entre altura predicha y altura medida por GEDI.

2. **C√°lculo de factor de reescalamiento**
   - Se cruzan las predicciones del modelo ALS y del modelo GEDI en zonas con datos comunes.
   - Se calcula un **factor de escala espacialmente suave** que corrige el CHM ALS.

3. **CHM final**
   - El CHM ALS de alta resoluci√≥n se multiplica por el factor de reescalamiento.
   - Resultado: **canopy height map continuo**, detallado y coherente con GEDI a escala global.

---



---


### 3. Descarga de pesos preentrenados ‚öñÔ∏èüå≥

Para que la aplicaci√≥n pueda realizar **inferencia real**, es indispensable descargar los **pesos preentrenados** del modelo original de Meta AI:  
**High-Resolution Canopy Height Maps**.



#### 3.1. ¬øDe d√≥nde descargar los pesos?

1. Ve al repositorio original del proyecto (Meta / `HighResolutionCanopyHeight`).
2. Busca la secci√≥n de **model checkpoints / weights**.
3. Descarga, como m√≠nimo, los siguientes archivos:

- ‚úÖ **Checkpoint del modelo CHM**, por ejemplo:  
  `compressed_SSLhuge_aerial.pth`

- ‚úÖ **Pesos de la red de normalizaci√≥n RNet**, usados cuando `normtype = 2`.  
  El nombre del archivo debe coincidir con lo que espera la funci√≥n  
  `load_rnet_normalizer()` en `model/ssl_model.py`.


#### 3.2. D√≥nde ubicar los archivos descargados

Copia los archivos descargados en la carpeta:

```bash
saved_checkpoints/
```

---

## üöÄ 4. Ejecuci√≥n del Proyecto con **Docker**
Instalaci√≥n ‚Ä¢ Despliegue ‚Ä¢ Uso

Este proyecto est√° preparado para ejecutarse f√°cilmente usando **Docker**, sin necesidad de instalar manualmente todas las dependencias en tu m√°quina local.



### üìÅ 4.1. Clonar el repositorio

```bash
git clone <URL_DE_TU_REPOSITORIO>
cd HighResCanopyHeightApp
```

‚ö†Ô∏è Importante:
Antes de continuar, aseg√∫rate de que la carpeta saved_checkpoints/ contiene los pesos indicados en la secci√≥n anterior (modelo CHM y RNet).


üõ†Ô∏è 4.2. Construir la imagen Docker
Desde la ra√≠z del proyecto, ejecuta:

```bash

docker build -t chm-demo .

```



üîé ¬øQu√© hace este comando?

Elemento	Descripci√≥n
-t chm-demo	Asigna el nombre chm-demo a la imagen Docker
.	Usa el Dockerfile ubicado en el directorio actual



üì¶ El Dockerfile se encarga de:

Instalar Python y las dependencias necesarias (PyTorch, PyTorch Lightning, Streamlit, etc.).

Copiar el c√≥digo fuente (app/, model/, utils/, etc.) dentro del contenedor.

Asegurar el acceso a saved_checkpoints/ para cargar los pesos del modelo.

Definir el comando de arranque de Streamlit como punto de entrada.




‚ñ∂Ô∏è 4.3. Ejecutar el contenedor
Una vez construida la imagen, puedes levantar el contenedor con:

```bash
docker run -p 8501:8501 chm-demo
```


üí° Si el puerto 8501 ya est√° ocupado en tu m√°quina, puedes usar otro puerto externo, por ejemplo:

```bash
docker run -p 8502:8501 chm-demo
```




üåê 4.4. Acceder a la aplicaci√≥n
Con el contenedor en ejecuci√≥n, abre tu navegador en:

```bash
http://localhost:8501
```
Deber√≠as ver la landing de la aplicaci√≥n.

Desde all√≠ puedes:

Navegar al modo ‚ÄúDemostraci√≥n‚Äù usando el men√∫ superior.

Explorar tiles reales del dataset NEON.

Visualizar la imagen a√©rea, el CHM real y el CHM predicho por el modelo.





## üíª 5. Ejecuci√≥n local (opcional, sin Docker)

Aunque la forma recomendada de ejecutar el proyecto es mediante **Docker**, tambi√©n puedes correr la aplicaci√≥n **localmente** si ya tienes **Python** instalado en tu m√°quina.

---

### üß¨ 5.1. Crear entorno virtual e instalar dependencias

Se recomienda usar un entorno virtual para aislar las dependencias del proyecto.

#### 1Ô∏è‚É£ Crear y activar el entorno virtual

```bash
python -m venv .venv
```

En Windows:

```bash
.venv\Scripts\activate
```

En Linux / macOS:

```bash

source .venv/bin/activate
```

Ver√°s que el prompt de tu terminal cambia, indicando que el entorno .venv est√° activo.

2Ô∏è‚É£ Actualizar pip e instalar dependencias
Con el entorno virtual activado, ejecuta:

```bash
pip install --upgrade pip
pip install -r requirements.txt

```

Esto instalar√° todas las librer√≠as necesarias para:

Cargar el modelo CHM y la red de normalizaci√≥n RNet.

Ejecutar la interfaz de Streamlit.

Trabajar con im√°genes, tensores y m√©tricas del modelo.

üöÄ 5.2. Lanzar la aplicaci√≥n con Streamlit
Una vez instaladas las dependencias, desde la ra√≠z del proyecto ejecuta:

```bash

streamlit run app/streamlit_landing_CHM_app.py

```

Si todo est√° correctamente configurado (incluyendo los pesos en saved_checkpoints/), Streamlit levantar√° la aplicaci√≥n.

üåê Acceder a la app
Abre tu navegador y visita:

```bash
http://localhost:8501
```

All√≠ podr√°s:

Ver la landing del proyecto.

Acceder al modo Demostraci√≥n.

Explorar los tiles del dataset NEON o las opciones que hayas habilitado en la app.


6. Explicaci√≥n: ¬øc√≥mo se cargan los pesos y c√≥mo se realiza la inferencia?

La l√≥gica de carga de pesos y de inferencia est√° dividida en dos contextos:

Modo NEON (dataset) ‚Äì usa RNet + NeonDataset.

Modo de imagen subida ‚Äì usa solo el modelo CHM con normalizaci√≥n global.

6.1. Modo NEON (dataset)

La l√≥gica principal est√° en model/inference_neon_tile.py y en la p√°gina app/pages/Demostraci√≥n.py.

6.1.1. Configuraci√≥n de componentes (setup_neon_inference)

En inference_neon_tile.py:

components = setup_neon_inference(
    checkpoint_name="compressed_SSLhuge_aerial.pth",
    normtype=2,
    trained_rgb=False,
    src_img="neon",
)


Esta funci√≥n:

Carga la red de normalizaci√≥n RNet (si normtype == 2) mediante:

model_norm = load_rnet_normalizer()


Construye el NeonDataset:

dataset = build_neon_dataset(
    model_norm=model_norm,
    normtype=normtype,
    trained_rgb=trained_rgb,
    src_img=src_img,
)


Aqu√≠ se aplica la normalizaci√≥n de dominio descrita en el paper para que las im√°genes NEON queden en un espacio similar al de entrenamiento del backbone.

Carga el modelo de altura de dosel (CHM):

model, device = load_chm_model(checkpoint_name=checkpoint_name)


Esto activa el modelo DINOv2 + DPT que predice alturas en metros.

Define la normalizaci√≥n global por canal:

norm = T.Normalize(
    mean=(0.420, 0.411, 0.296),
    std=(0.213, 0.156, 0.143),
)


Es la misma normalizaci√≥n utilizada en el script de inferencia original.

El resultado es un diccionario:

components = {
    "model": model,
    "device": device,
    "dataset": dataset,
    "norm": norm,
}


que la app reutiliza para todos los tiles.

6.1.2. Inferencia sobre un tile (run_neon_tile_inference)

Cuando el usuario selecciona un √≠ndice y pulsa ‚Äú‚ö° Calcular CHM para este tile‚Äù, en Demostraci√≥n.py se llama:

result = run_neon_tile_inference(components, idx)


Dentro de run_neon_tile_inference:

Obtiene el sample del dataset:

img_no_norm, img_norm, chm = get_neon_sample(dataset, index)


img_no_norm: imagen RGB original.

img_norm: imagen ya ajustada por RNet / normalizaci√≥n de dominio.

chm: CHM real (LiDAR).

Prepara el batch e incluye la normalizaci√≥n global:

x = img_norm.unsqueeze(0)  # [1, 3, H, W]
x = norm(x)
x = x.to(device)


Ejecuta el modelo CHM:

model.eval()
with torch.no_grad():
    pred = model(x)          # [1, 1, H, W]
    pred = pred.cpu().relu()
    pred_map = pred[0, 0].numpy()  # [H, W]


Recupera el CHM real:

chm_map = chm[0].numpy()


Calcula las m√©tricas:

metrics = compute_all_metrics(pred_map, chm_map)


Que incluye: MAE, RMSE, R¬≤ pixel, R¬≤ por bloques, Bias, etc.

Prepara la imagen RGB para mostrarla:

img_rgb = np.moveaxis(img_no_norm.numpy(), 0, 2)  # [H, W, 3]


Devuelve:

result = {
    "img_rgb": img_rgb,
    "chm_gt": chm_map,
    "chm_pred": pred_map,
    "metrics": metrics,
}


En la app, chm_pred y chm_gt se normalizan a [0,1] y se convierten a mapas de color con un colormap tipo viridis para mostrarlos como im√°genes.

### üñºÔ∏è 6.2. Modo de imagen subida

En este modo **no se usa RNet**: se asume que las im√°genes subidas por el usuario son razonablemente similares al dominio NEON (im√°genes a√©reas, alta resoluci√≥n, etc.).

La l√≥gica principal est√° en el bloque `else:` de:

- `app/pages/Demostraci√≥n.py`

---

#### ‚öôÔ∏è 6.2.1. Carga del modelo

Para este modo se prepara un conjunto de componentes m√°s simple:

```python
model, device = load_chm_model(checkpoint_name="compressed_SSLhuge_aerial.pth")

norm = T.Normalize(
    mean=(0.420, 0.411, 0.296),
    std=(0.213, 0.156, 0.143),
)
En resumen:

Se carga el modelo CHM (backbone DINOv2 + decoder DPT).

Se define la normalizaci√≥n global por canal, igual a la usada en el script de inferencia original.

No se construye NeonDataset ni se aplica RNet.

üîÅ 6.2.2. Flujo de inferencia
El usuario puede subir:

rgb_file: imagen a√©rea RGB.

chm_file (opcional): raster de CHM real, co-registrado con la imagen RGB.

1Ô∏è‚É£ Procesamiento de la imagen RGB
La imagen se transforma a tensor normalizado antes de entrar al modelo:

python
Copiar c√≥digo
rgb_img = Image.open(rgb_file).convert("RGB")
img_np = np.array(rgb_img).astype("float32") / 255.0  # [H, W, 3]

img_t = torch.from_numpy(img_np).permute(2, 0, 1)     # [3, H, W]
x = img_t.unsqueeze(0)                                # [1, 3, H, W]
x = norm(x).to(device)
Pasos clave:

Se abre la imagen y se asegura el modo RGB.

Se normaliza a rango [0, 1].

Se permutan las dimensiones a formato [C, H, W].

Se a√±ade la dimensi√≥n de batch ‚Üí [1, 3, H, W].

Se aplica la normalizaci√≥n global norm y se env√≠a a la device.

2Ô∏è‚É£ Predicci√≥n del CHM
Se ejecuta el modelo para obtener el mapa de altura predicho:

python
Copiar c√≥digo
with torch.no_grad():
    pred = model(x)
    pred = pred.cpu().relu()[0, 0].numpy()  # [H, W]

chm_pred_up = pred
Se desactiva el gradiente (torch.no_grad()).

El modelo devuelve un tensor [1, 1, H, W].

Se lleva a CPU, se aplica relu() (sin alturas negativas) y se extrae el mapa [H, W].

3Ô∏è‚É£ (Opcional) Uso de un CHM real para evaluaci√≥n
Si el usuario tambi√©n sube un archivo de CHM real:

python
Copiar c√≥digo
chm_img = Image.open(chm_file)
chm_arr = np.array(chm_img).astype("float32")

if chm_arr.ndim == 3:
    chm_arr = chm_arr[..., 0]

if chm_arr.shape != chm_pred_up.shape:
    raise ValueError(
        f"Dimensiones distintas entre predicci√≥n {chm_pred_up.shape} "
        f"y CHM real {chm_arr.shape}. Deben coincidir."
    )
Se carga el raster de CHM.

Si viene con 3 canales, se toma solo uno.

Se valida que el tama√±o del CHM real coincida con el de la predicci√≥n; si no, se lanza un error.

Solo cuando las dimensiones coinciden se calculan las m√©tricas:

python
Copiar c√≥digo
metrics = compute_all_metrics(chm_pred_up, chm_arr)
üëÄ 6.2.3. Qu√© muestra la app en este modo
La interfaz visualiza:

‚úÖ Imagen RGB subida por el usuario.

‚úÖ CHM predicho por el modelo (convertido a mapa de color).

‚úÖ CHM real, si fue proporcionado y tiene el mismo tama√±o.

‚úÖ Una tabla de m√©tricas (MAE, RMSE, R¬≤, Bias, etc.) cuando se proporciona un CHM real v√°lido.

De esta manera, el usuario puede:

Probar el modelo con sus propias im√°genes.

Comparar la predicci√≥n del modelo contra un CHM real (si lo tiene).

Evaluar cuantitativamente el desempe√±o mediante las m√©tricas mostradas en la app



---

## üìö Referencias

- Weinstein, B. G., et al. **High-resolution canopy height maps by learning from airborne lidar and spaceborne GEDI.**  
- Repositorio oficial: https://github.com/facebookresearch/HighResCanopyHeight
- Oquab, M., et al. **DINOv2: Learning robust visual features without supervision.**
- Ranftl, R., et al. **Vision Transformers for dense prediction (DPT).**
